{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification with Attention\n",
    "\n",
    "We use the dataset and embeddings from a previous problem to demonstrate attention in TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import/Initial Processing:\n",
    "\n",
    "Note: `encoding = \"ISO-8859-1\"` in `pd.read_csv` deals with UTF errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>SUMMARY</th>\n",
       "      <th>DATA</th>\n",
       "      <th>categories</th>\n",
       "      <th>sub_categories</th>\n",
       "      <th>previous_appointment</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015561331001</td>\n",
       "      <td>Pt aware that he needs ROV for refill</td>\n",
       "      <td>{\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...</td>\n",
       "      <td>PRESCRIPTION</td>\n",
       "      <td>REFILL</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6133_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015561341001</td>\n",
       "      <td>Mom wants to know if the Focalin needs some do...</td>\n",
       "      <td>{\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...</td>\n",
       "      <td>ASK_A_DOCTOR</td>\n",
       "      <td>MEDICATION RELATED</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6134_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015561351001</td>\n",
       "      <td>pt called to discuss nortryptiline. she says s...</td>\n",
       "      <td>xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...</td>\n",
       "      <td>ASK_A_DOCTOR</td>\n",
       "      <td>MEDICATION RELATED</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6135_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015561361001</td>\n",
       "      <td>FYI Nortryptline medication.</td>\n",
       "      <td>xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6136_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015561371001</td>\n",
       "      <td>Letter of patient establishment request</td>\n",
       "      <td>{\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6137_1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fileid                                            SUMMARY  \\\n",
       "0  2015561331001              Pt aware that he needs ROV for refill   \n",
       "1  2015561341001  Mom wants to know if the Focalin needs some do...   \n",
       "2  2015561351001  pt called to discuss nortryptiline. she says s...   \n",
       "3  2015561361001                       FYI Nortryptline medication.   \n",
       "4  2015561371001            Letter of patient establishment request   \n",
       "\n",
       "                                                DATA     categories  \\\n",
       "0  {\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...   PRESCRIPTION   \n",
       "1  {\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...   ASK_A_DOCTOR   \n",
       "2  xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...   ASK_A_DOCTOR   \n",
       "3  xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...  MISCELLANEOUS   \n",
       "4  {\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...  MISCELLANEOUS   \n",
       "\n",
       "                                  sub_categories previous_appointment  \\\n",
       "0                                         REFILL                   No   \n",
       "1                             MEDICATION RELATED                   No   \n",
       "2                             MEDICATION RELATED                   No   \n",
       "3                                         OTHERS                   No   \n",
       "4  SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)                   No   \n",
       "\n",
       "                 ID  \n",
       "0  2015_5_6133_1001  \n",
       "1  2015_5_6134_1001  \n",
       "2  2015_5_6135_1001  \n",
       "3  2015_5_6136_1001  \n",
       "4  2015_5_6137_1001  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data = pd.read_csv('Text_Class_Prototype//TextClassification_Data.csv', encoding = \"ISO-8859-1\")\n",
    "pre_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dataset to text and action category, drop unecessary data points and fix category errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PRESCRIPTION': 14500, 'APPOINTMENTS': 12960, 'ASK_A_DOCTOR': 11744, 'MISCELLANEOUS': 10463, 'LAB': 4246})\n"
     ]
    }
   ],
   "source": [
    "pre_data = pre_data[['SUMMARY', 'categories']].dropna()\n",
    "def fix(data_frame):\n",
    "    junk_list = list(data_frame[data_frame['categories'] == 'JUNK'].index)\n",
    "    data_frame = data_frame.drop(junk_list, axis = 0)\n",
    "    data_frame = data_frame.replace(['mISCELLANEOUS', 'asK_A_DOCTOR'], ['MISCELLANEOUS','ASK_A_DOCTOR'])\n",
    "    return data_frame\n",
    "\n",
    "pre_data = fix(pre_data)\n",
    "print(collections.Counter(pre_data['categories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Attention \n",
    "\n",
    "### Forward Only and BiDirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained Word2Vec 5-gram embeddings.\n",
    "\n",
    "def load_embeddings(embedding_name):\n",
    "    # Load embeddings.\n",
    "    embeddings_path = os.path.join(os.path.join(os.getcwd(), embedding_name), embedding_name + \".npy\")\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    date_time_stamp = embedding_name[5:]\n",
    "    path_str = str(os.getcwd()) + \"/\" + embedding_name + \"/\"\n",
    "    # Load dictionary.\n",
    "    dict_path = path_str + \"Dictionary\" + date_time_stamp + \".pickle\"\n",
    "    with open(dict_path, \"rb\") as filename:\n",
    "        dictionary = pickle.load(filename)\n",
    "    # Load reverse dictionary.\n",
    "    rev_dict_path = path_str + \"RevDictionary\" + date_time_stamp + \".pickle\"\n",
    "    with open(rev_dict_path, \"rb\") as filename:\n",
    "        reverse_dictionary = pickle.load(filename)\n",
    "    return embeddings, dictionary, reverse_dictionary\n",
    "\n",
    "embeddings, dictionary, reverse_dictionary = load_embeddings(\"Embed_2019-10-06_16.57.49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.54758091e-02,  4.42251340e-02, -1.04927778e-01, -6.09349944e-02,\n",
       "        3.98972146e-02,  9.31131281e-03, -1.25223726e-01,  7.53500080e-03,\n",
       "       -1.38559267e-01, -1.09519631e-01,  5.10512032e-02, -4.92008999e-02,\n",
       "        1.50272110e-02, -8.88878256e-02, -5.26870079e-02,  1.06981238e-02,\n",
       "       -3.02361138e-02,  3.60170864e-02,  1.22638404e-01,  3.42754312e-02,\n",
       "        1.66808501e-01,  9.66842175e-02, -1.10033818e-01,  9.19144452e-02,\n",
       "        1.57749012e-01, -7.89843574e-02, -6.32723495e-02,  3.49363312e-02,\n",
       "       -3.63217667e-02,  1.63602401e-02,  1.64437354e-01, -2.45347177e-03,\n",
       "        3.99290361e-02, -2.66418164e-03,  3.39282607e-03, -3.54395360e-02,\n",
       "        8.31402689e-02, -6.25631437e-02,  4.52763308e-03, -6.81839585e-02,\n",
       "       -5.21346368e-02,  8.64811316e-02, -4.14496334e-03,  7.28750825e-02,\n",
       "       -7.67338574e-02, -6.07562065e-03,  3.65152992e-02, -1.11119702e-01,\n",
       "        7.68361986e-02, -9.15446039e-03, -8.86462480e-02,  6.94847032e-02,\n",
       "        7.60806128e-02, -1.74372289e-02,  2.95694415e-02, -1.55135930e-01,\n",
       "       -1.47924602e-01, -2.69719586e-02, -1.52396215e-02, -2.62599569e-02,\n",
       "       -2.44120937e-02, -1.33610129e-01, -9.15344059e-02, -1.94220111e-01,\n",
       "        7.17281848e-02, -1.13016568e-01, -3.29790935e-02,  3.12414356e-02,\n",
       "        6.60010576e-02, -5.50135411e-02,  8.64154249e-02,  2.53950357e-02,\n",
       "       -7.96117857e-02,  6.85733627e-04,  1.24127530e-01,  1.89189523e-01,\n",
       "       -8.84971172e-02, -1.19952092e-04, -1.10718057e-01, -8.02403688e-02,\n",
       "        2.86105182e-02, -4.30388078e-02, -5.07645048e-02, -7.23842680e-02,\n",
       "       -1.03527233e-01,  1.15440756e-01,  2.67750248e-02, -2.20735651e-02,\n",
       "        3.00977658e-02,  6.59847446e-03,  1.02196902e-01,  1.28726229e-01,\n",
       "        7.67759085e-02, -5.68365827e-02,  1.62440464e-02, -2.04598889e-01,\n",
       "        9.18036699e-02, -7.80110131e-05, -7.15658292e-02, -9.72000360e-02,\n",
       "        1.70929253e-01, -6.03064708e-03, -1.25637785e-01,  9.46558490e-02,\n",
       "        6.15262017e-02,  6.70119226e-02, -9.44705680e-02, -1.83322206e-01,\n",
       "       -1.23451566e-02, -1.79034024e-02, -2.85494756e-02, -1.04825996e-01,\n",
       "       -5.27595691e-02, -8.94923732e-02, -4.47466634e-02,  9.49782040e-03,\n",
       "        9.13989246e-02, -2.35021506e-02, -1.55204684e-01,  8.57206807e-02,\n",
       "       -9.97810885e-02, -6.81907907e-02,  1.06277995e-01,  8.31815302e-02,\n",
       "       -8.10356066e-02,  4.63768952e-02,  3.20618719e-01, -1.13388680e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that embeddings and dictionary loaded properly. \n",
    "\n",
    "embeddings[dictionary['refill']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pre_data.values[...,0]\n",
    "text_labels = pre_data.values[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful label encoding.\n",
      "Number of classes: 5\n",
      "Number of labels: 53913\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(labels, print_stats = True):\n",
    "    unique_labels = np.unique(labels)\n",
    "    identity = (unique_labels == unique_labels[:,None]).astype(np.float32)\n",
    "    lookup_dict = dict(zip(unique_labels.tolist(), [identity[i] for i in range(identity.shape[0])]))\n",
    "    out_labels = (unique_labels == labels[:,None]).astype(np.float32)\n",
    "    def lookup_function(label):\n",
    "        return lookup_dict[label]\n",
    "    def reverse_lookup_function(encoded_label):\n",
    "        rev_look = [x for x,y in lookup_dict.items() if np.array_equal(y, encoded_label)]\n",
    "        return rev_look[0]\n",
    "    def decode_labels(encoded_label_array):\n",
    "        return np.array([reverse_lookup_function(encoded_label_array[i]) for i in range(encoded_label_array.shape[0])])\n",
    "    if print_stats:\n",
    "        if np.array_equal(decode_labels(out_labels), labels):\n",
    "            print('Successful label encoding.')\n",
    "            print('Number of classes: ' + str(unique_labels.shape[0]))\n",
    "            print('Number of labels: ' + str(out_labels.shape[0]))\n",
    "        else:\n",
    "            print('Bijection failure.')\n",
    "    return out_labels, lookup_function, reverse_lookup_function, decode_labels\n",
    "\n",
    "labels, label2vec, vec2label, decode_labels = encode_labels(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting complete.\n"
     ]
    }
   ],
   "source": [
    "stop_list = [\".\",\",\",\"!\",\"?\",\":\",\";\",\"/\",\"(\",\")\",\"-\", \"\\ufeff\", \"\\\"\", \"\\'s\"]\n",
    "measurements = [\"mg\", \"cc\", \"lb\", \"lbs\", \"kg\", \"kgs\"]\n",
    "end_signature = '[..end..]'\n",
    "\n",
    "def format_string(string):\n",
    "    global stop_list\n",
    "    global measurements\n",
    "    for item in stop_list:\n",
    "        string = string.replace(item, \" \" + item + \" \")\n",
    "    for item in measurements:\n",
    "        if item in string:\n",
    "            index = string.find(item)\n",
    "            if ord(string[index - 1]) in range(48,58):\n",
    "                string = string[:index] + \" \" + string[index:]\n",
    "    return string.lower()\n",
    "\n",
    "def format_data_LSTM(data):\n",
    "    new_data = copy.deepcopy(data)\n",
    "    for i in range(len(data)):\n",
    "        new_data[i] = np.array([dictionary[x] for x in format_string(new_data[i]).split()])\n",
    "    print(\"Formatting complete.\")\n",
    "    return new_data\n",
    "\n",
    "data = format_data_LSTM(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt aware that he needs rov for refill <end>\n",
      "mom wants to know if the focalin needs some dosage adjusting <end>\n",
      "pt called to discuss nortryptiline . she says she has a weird tas <end>\n",
      "fyi nortryptline medication . <end>\n",
      "letter of patient establishment request <end>\n"
     ]
    }
   ],
   "source": [
    "# Sanity check.\n",
    "\n",
    "for j in range(5):\n",
    "    for i in range(len(data[j])):\n",
    "        print(reverse_dictionary[data[j][i]], end = \" \")\n",
    "    print(\"<end>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Sequences for LSTM\n",
    "\n",
    "Each string formatted as array of embedding lookup indices (integers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Origin to Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = int(embeddings.shape[1])\n",
    "dictionary[\"\"] = -1\n",
    "reverse_dictionary[-1] = \"\"\n",
    "origin = np.zeros((embedding_dimension,))\n",
    "embeddings = np.append(embeddings, np.array([origin]), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# Find max length of sequence in data.\n",
    "\n",
    "max_length = 0\n",
    "for i in range(len(data)):\n",
    "    if len(data[i]) > max_length:\n",
    "        max_length = len(data[i])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_padding(sequence_array):\n",
    "    global max_length\n",
    "    out_array = copy.deepcopy(sequence_array)\n",
    "    for i in range(len(out_array)):\n",
    "        difference = max_length - len(out_array[i])\n",
    "        fill = np.full((1, difference), -1, dtype=int)\n",
    "        out_array[i] = np.concatenate((fill, out_array[i]), axis = None)\n",
    "    return out_array\n",
    "\n",
    "def right_padding(sequence_array):\n",
    "    global max_length\n",
    "    out_array = copy.deepcopy(sequence_array)\n",
    "    for i in range(len(out_array)):\n",
    "        difference = max_length - len(out_array[i])\n",
    "        fill = np.full((1, difference), -1, dtype=int)\n",
    "        out_array[i] = np.concatenate((out_array[i], fill), axis = None)\n",
    "    return out_array\n",
    "\n",
    "left_padded_data = left_padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt aware that he needs rov for refill <end>\n",
      "mom wants to know if the focalin needs some dosage adjusting <end>\n",
      "pt called to discuss nortryptiline . she says she has a weird tas <end>\n",
      "fyi nortryptline medication . <end>\n",
      "letter of patient establishment request <end>\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check. \n",
    "\n",
    "for j in range(5):\n",
    "    for i in range(len(left_padded_data[j])):\n",
    "        if reverse_dictionary[left_padded_data[j][i]] != \"\":\n",
    "            print(reverse_dictionary[left_padded_data[j][i]], end = \" \")\n",
    "    print(\"<end>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Train/Valid/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(data_array, label_array, \n",
    "                           train_test_ratio = .85, \n",
    "                           train_valid_ratio = .85, \n",
    "                           shuffle_count = 7,\n",
    "                           print_shapes = True):\n",
    "    labels = np.unique(label_array, axis = 0) # An array of the distinct entry values occuring in the argument.\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    valid_indices = []\n",
    "    for label in labels.tolist():\n",
    "        label_indices = [i for i,x in enumerate(label_array) if np.array_equal(x, label)]\n",
    "        instance_count = len(label_indices)\n",
    "        train_test_partition = int(math.floor(train_test_ratio*instance_count))\n",
    "        random.shuffle(label_indices)\n",
    "        to_train_valid = label_indices[:train_test_partition]\n",
    "        to_test = label_indices[train_test_partition:]\n",
    "        test_indices = test_indices + to_test\n",
    "        train_valid_partition = int(math.floor(train_valid_ratio*len(to_train_valid)))\n",
    "        to_train = to_train_valid[:train_valid_partition]\n",
    "        to_valid = to_train_valid[train_valid_partition:]\n",
    "        train_indices = train_indices + to_train\n",
    "        valid_indices = valid_indices + to_valid\n",
    "    for i in range(shuffle_count):\n",
    "        random.shuffle(train_indices)\n",
    "        random.shuffle(valid_indices)\n",
    "        random.shuffle(test_indices)\n",
    "    train_data = np.stack(data_array[train_indices], axis = 0).astype(int)\n",
    "    train_labels = label_array[train_indices]\n",
    "    valid_data = np.stack(data_array[valid_indices], axis = 0).astype(int)\n",
    "    valid_labels = label_array[valid_indices]\n",
    "    test_data = np.stack(data_array[test_indices], axis = 0).astype(int)\n",
    "    test_labels = label_array[test_indices]\n",
    "    if print_shapes:\n",
    "        print(\"Train data shape: \" + str(train_data.shape))\n",
    "        print(\"Train labels shape: \" + str(train_labels.shape))\n",
    "        print(\"Valid data shape: \" + str(valid_data.shape))\n",
    "        print(\"Valid labels shape: \" + str(valid_labels.shape))\n",
    "        print(\"Test data shape: \" + str(test_data.shape))\n",
    "        print(\"Test labels shape: \" + str(test_labels.shape))\n",
    "    return train_data, train_labels, valid_data, valid_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (38949, 28)\n",
      "Train labels shape: (38949, 5)\n",
      "Valid data shape: (6876, 28)\n",
      "Valid labels shape: (6876, 5)\n",
      "Test data shape: (8088, 28)\n",
      "Test labels shape: (8088, 5)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, valid_data, valid_labels, test_data, test_labels = train_valid_test_split(left_padded_data,\n",
    "                                                                                                    labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Batch Generator\n",
    "\n",
    "Thought: change this all to maintain one data array and simply shuffle the indices of drawn data across epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "number_of_unrollings = max_length \n",
    "number_of_classes = len(labels[0])\n",
    "embedding_dimension = len(embeddings[0])\n",
    "data_index = 0\n",
    "\n",
    "def generate_LSTM_batch(data, labels, batch_size, number_of_unrollings, embedding_dimension):\n",
    "    global data_index\n",
    "    data_index = data_index % len(data)\n",
    "    batch = np.zeros(shape=(batch_size, number_of_unrollings, embedding_dimension), dtype=np.float32)\n",
    "    labels_out = np.zeros(shape=(batch_size, number_of_classes), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        batch[i] = embeddings[data[data_index].astype(int)].reshape(1, number_of_unrollings, embedding_dimension)\n",
    "        labels_out[i] = labels[data_index]\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels_out\n",
    "\n",
    "valid_total, valid_total_labels = generate_LSTM_batch(valid_data, \n",
    "                                                      valid_labels, \n",
    "                                                      len(valid_data), \n",
    "                                                      number_of_unrollings, \n",
    "                                                      embedding_dimension)\n",
    "\n",
    "test_total, test_total_labels = generate_LSTM_batch(test_data, \n",
    "                                                    test_labels, \n",
    "                                                    len(test_data), \n",
    "                                                    number_of_unrollings, \n",
    "                                                    embedding_dimension)\n",
    "\n",
    "data_index = 14\n",
    "example_sentence = train_data[data_index]\n",
    "example_data, example_label = generate_LSTM_batch(train_data, \n",
    "                                                  train_labels, \n",
    "                                                  1, \n",
    "                                                  number_of_unrollings, \n",
    "                                                  embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Scoring Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_accuracy(predicted_labels, true_labels):\n",
    "    return(100*np.sum(np.argmax(predicted_labels, 1)==np.argmax(true_labels, 1))\n",
    "           /predicted_labels.shape[0])\n",
    "\n",
    "def vanish_max_axis1(array):\n",
    "    index = np.argmax(array, 1)\n",
    "    for i in range(len(array)):\n",
    "        array[i][index[i]] = 0\n",
    "    return array\n",
    "\n",
    "def second_order_accuracy(predicted_labels, true_labels):\n",
    "    second_predicted_labels = vanish_max_axis1(predicted_labels)\n",
    "    return(100*np.sum(np.argmax(second_predicted_labels, 1)==np.argmax(true_labels, 1))\n",
    "           /predicted_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward LSTM with Attention Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters ###\n",
    "number_of_nodes = 64\n",
    "initial_learning_rate = 1.\n",
    "init_truncation = .01\n",
    "#######################\n",
    "\n",
    "LSTM_UniAttn_graph = tf.Graph()\n",
    "with LSTM_UniAttn_graph.as_default():\n",
    "    # Input\n",
    "    T = tf.placeholder(tf.float32, shape = [batch_size, number_of_unrollings, embedding_dimension])\n",
    "    Tlabel = tf.placeholder(tf.float32, shape = [batch_size, number_of_classes])\n",
    "    V = tf.constant(valid_total)\n",
    "    E = tf.constant(test_total)\n",
    "    \n",
    "    # LSTM variables\n",
    "    # Input gate.\n",
    "    X_i = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_i = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_i = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Forget gate.\n",
    "    X_f = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_f = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_f = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Memory cell.                             \n",
    "    X_c = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_c = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_c = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Output gate.\n",
    "    X_o = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_o = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_o = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    \n",
    "    # Attention variables.\n",
    "    # Context vector.\n",
    "    C_attn = tf.Variable(tf.truncated_normal([number_of_nodes//2 , 1], -init_truncation, init_truncation))\n",
    "    # Attention weights and biases.\n",
    "    W_attn = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes//2], -init_truncation, init_truncation))\n",
    "    b_attn = tf.Variable(tf.zeros([number_of_nodes//2]))\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    W = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_classes], -init_truncation, init_truncation))\n",
    "    b = tf.Variable(tf.zeros([number_of_classes]))\n",
    "    \n",
    "    # Initialize time 0 state and previous output. \n",
    "    initial_output = tf.Variable(tf.zeros([batch_size, number_of_nodes]), trainable=False)\n",
    "    initial_state = tf.Variable(tf.zeros([batch_size, number_of_nodes]), trainable=False)\n",
    "     \n",
    "    def LSTM_cell(sequence_element, previous_output, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(sequence_element, X_i) + tf.matmul(previous_output, M_i) + b_i)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(sequence_element, X_f) + tf.matmul(previous_output, M_f) + b_f)\n",
    "        update = tf.matmul(sequence_element, X_c) + tf.matmul(previous_output, M_c) + b_c\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(sequence_element, X_o) + tf.matmul(previous_output, M_o) + b_o)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    def LSTM_loop(data, number_of_unrollings, initial_output, initial_state):\n",
    "        outputs = list()\n",
    "        output = initial_output\n",
    "        state = initial_state\n",
    "        for i in range(number_of_unrollings):\n",
    "            output, state = LSTM_cell(data[:,i], output, state)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    # Attention model:\n",
    "    def attention_layer(outputs, W_attn, b_attn, C_attn):\n",
    "        U = list()\n",
    "        for i in range(len(outputs)):\n",
    "            u = tf.tanh(tf.nn.xw_plus_b(outputs[i], W_attn, b_attn))\n",
    "            U.append(u)\n",
    "    \n",
    "        C = list()\n",
    "        for i in range(len(U)):\n",
    "            c = tf.matmul(U[i], C_attn)\n",
    "            C.append(c)\n",
    "    \n",
    "        C_concat = tf.concat(C, 1)\n",
    "        S = tf.nn.softmax(C_concat, 1)\n",
    "        \n",
    "        attn_outputs = list()\n",
    "        for i in range(number_of_unrollings):\n",
    "            attn_output = tf.multiply(outputs[i], S[:,i][:, tf.newaxis])\n",
    "            attn_outputs.append(attn_output)\n",
    "        return attn_outputs, S\n",
    "    \n",
    "    outputs = LSTM_loop(T, number_of_unrollings, initial_output, initial_state)\n",
    "    attn_outputs, S = attention_layer(outputs, W_attn, b_attn, C_attn)\n",
    "    # attn_outputs, last_attn = attention_layer(outputs, W_attn, b_attn, C_attn)\n",
    "    # Compute logits and loss for attention-weighted sum of outputs.\n",
    "    L = tf.nn.xw_plus_b(tf.math.add_n(attn_outputs), W, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = Tlabel, logits=L))\n",
    "    \n",
    "    # Decaying learning rate for optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)    \n",
    "    \n",
    "    # Apply gradient clipping. \n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Training predictions. \n",
    "    train_predict = tf.nn.softmax(L)\n",
    "    \n",
    "    # Test predictions. \n",
    "    # Initialize output, state at origin. \n",
    "    test_initial_output = tf.Variable(tf.zeros([len(test_total), number_of_nodes]), trainable = False)\n",
    "    test_initial_state = tf.Variable(tf.zeros([len(test_total), number_of_nodes]), trainable = False)\n",
    "    \n",
    "    test_outputs = LSTM_loop(E, number_of_unrollings, test_initial_output, test_initial_state)\n",
    "    test_attn_outputs, _ = attention_layer(test_outputs, W_attn, b_attn, C_attn)\n",
    "    test_predict = tf.nn.softmax(tf.nn.xw_plus_b(tf.math.add_n(test_attn_outputs), W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1001/1001 [00:11<00:00, 87.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Test Report ======\n",
      "First-order accuracy: 73.2%\n",
      "Second-order accuracy: 14.7%\n",
      "Accuracy when first and second choice are counted: 87.9%\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations = 1001\n",
    "data_index = 0\n",
    "\n",
    "with tf.Session(graph=LSTM_UniAttn_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for iteration in tqdm(range(number_of_iterations)):\n",
    "        batch_data, batch_labels = generate_LSTM_batch(train_data, \n",
    "                                                       train_labels, \n",
    "                                                       batch_size, \n",
    "                                                       number_of_unrollings, \n",
    "                                                       embedding_dimension)\n",
    "        feed_dict = {T : batch_data, Tlabel : batch_labels}\n",
    "        _, l, train_predictions, lr = session.run([optimizer, loss, train_predict, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if iteration == 10:\n",
    "            ten_attention_coeff = S.eval(feed_dict = feed_dict)\n",
    "        \n",
    "    last_attention_coeff = S.eval(feed_dict = feed_dict)\n",
    "    test_predictions = test_predict.eval()\n",
    "    \n",
    "    test_acc1 = first_order_accuracy(test_predictions, test_total_labels)\n",
    "    test_acc2 = second_order_accuracy(test_predictions, test_total_labels)\n",
    "    print('====== Test Report ======')\n",
    "    print('First-order accuracy: %.1f%%' % test_acc1)\n",
    "    print('Second-order accuracy: %.1f%%' % test_acc2)\n",
    "    print('Accuracy when first and second choice are counted: %.1f%%' % (test_acc1+test_acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attention matrix:\n",
      "(64, 28)\n",
      "\n",
      "Iteration 10:\n",
      "\n",
      "Sum / Max / Argmax\n",
      "1.0 / 0.035736352 / 27\n",
      "\n",
      "0.9999999 / 0.03572114 / 27\n",
      "\n",
      "0.9999999 / 0.035744026 / 26\n",
      "\n",
      "Last iteration:\n",
      "\n",
      "Sum / Max / Argmax\n",
      "0.99999976 / 0.8471151 / 18\n",
      "\n",
      "1.0 / 0.7061416 / 27\n",
      "\n",
      "1.0 / 0.70872587 / 16\n",
      "\n",
      "Attention weights examples:\n",
      "[0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.114, 0.847, 0.033, 0.0, 0.001, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.003, 0.001, 0.001, 0.0, 0.0, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.277, 0.706]\n",
      "[0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.147, 0.035, 0.004, 0.709, 0.001, 0.009, 0.0, 0.022, 0.002, 0.051, 0.001, 0.0, 0.003, 0.009, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check.\n",
    "\n",
    "print(\"Shape of attention matrix:\")\n",
    "print(last_attention_coeff.shape, end = '\\n\\n')\n",
    "\n",
    "print(\"Iteration 10:\", end = \"\\n\\n\")\n",
    "print(\"Sum / Max / Argmax\")\n",
    "for i in range(3):\n",
    "    print(np.sum(ten_attention_coeff[i]), end =\" / \")\n",
    "    print(np.max(ten_attention_coeff[i]), end =\" / \")\n",
    "    print(np.argmax(ten_attention_coeff[i]), end = \"\\n\\n\")\n",
    "\n",
    "print(\"Last iteration:\", end = \"\\n\\n\")\n",
    "print(\"Sum / Max / Argmax\")\n",
    "for i in range(3):\n",
    "    print(np.sum(last_attention_coeff[i]), end =\" / \")\n",
    "    print(np.max(last_attention_coeff[i]), end =\" / \")\n",
    "    print(np.argmax(last_attention_coeff[i]), end = \"\\n\\n\")\n",
    "\n",
    "print(\"Attention weights examples:\")\n",
    "for i in range(3):\n",
    "    print([round(entry, 3) for entry in last_attention_coeff[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with Attention Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters ###\n",
    "number_of_nodes = 64\n",
    "initial_learning_rate = 1.\n",
    "init_truncation = .1\n",
    "#######################\n",
    "\n",
    "BiLSTM_graph = tf.Graph()\n",
    "with BiLSTM_graph.as_default():\n",
    "    # Input\n",
    "    T = tf.placeholder(tf.float32, shape = [batch_size, number_of_unrollings, embedding_dimension])\n",
    "    Tlabel = tf.placeholder(tf.float32, shape = [batch_size, number_of_classes])\n",
    "    V = tf.constant(valid_total)\n",
    "    E = tf.constant(test_total)\n",
    "    \n",
    "    # Forward LSTM variables\n",
    "    # Input gate.\n",
    "    X_if = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_if = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_if = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Forget gate.\n",
    "    X_ff = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_ff = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_ff = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Memory cell.                             \n",
    "    X_cf = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_cf = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_cf = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Output gate.\n",
    "    X_of = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_of = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_of = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    \n",
    "    # Backward LSTM variables\n",
    "    # Input gate.\n",
    "    X_ib = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_ib = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_ib = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Forget gate.\n",
    "    X_fb = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_fb = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_fb = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Memory cell.                             \n",
    "    X_cb = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_cb = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_cb = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    # Output gate.\n",
    "    X_ob = tf.Variable(tf.truncated_normal([embedding_dimension, number_of_nodes], -init_truncation, init_truncation))\n",
    "    M_ob = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_ob = tf.Variable(tf.zeros([1, number_of_nodes]))\n",
    "    \n",
    "    # Attention variables.\n",
    "    # Context vector.\n",
    "    C_attn = tf.Variable(tf.truncated_normal([number_of_nodes , 1], -init_truncation, init_truncation))\n",
    "    # Attention weights and biases.\n",
    "    W_attn = tf.Variable(tf.truncated_normal([2*number_of_nodes, number_of_nodes], -init_truncation, init_truncation))\n",
    "    b_attn = tf.Variable(tf.zeros([number_of_nodes]))\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    W = tf.Variable(tf.truncated_normal([2*number_of_nodes, number_of_classes], -init_truncation, init_truncation))\n",
    "    b = tf.Variable(tf.zeros([number_of_classes]))\n",
    "    \n",
    "    # Initialize time 0 state and previous output. \n",
    "    initial_output = tf.Variable(tf.zeros([batch_size, number_of_nodes]), trainable=False)\n",
    "    initial_state = tf.Variable(tf.zeros([batch_size, number_of_nodes]), trainable=False)\n",
    "     \n",
    "    def forward_LSTM_cell(sequence_element, previous_output, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(sequence_element, X_if) + tf.matmul(previous_output, M_if) + b_if)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(sequence_element, X_ff) + tf.matmul(previous_output, M_ff) + b_ff)\n",
    "        update = tf.matmul(sequence_element, X_cf) + tf.matmul(previous_output, M_cf) + b_cf\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(sequence_element, X_of) + tf.matmul(previous_output, M_of) + b_of)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def backward_LSTM_cell(sequence_element, previous_output, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(sequence_element, X_ib) + tf.matmul(previous_output, M_ib) + b_ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(sequence_element, X_fb) + tf.matmul(previous_output, M_fb) + b_fb)\n",
    "        update = tf.matmul(sequence_element, X_cb) + tf.matmul(previous_output, M_cb) + b_cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(sequence_element, X_ob) + tf.matmul(previous_output, M_ob) + b_ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Unrolled LSTM loops.\n",
    "    def forward_LSTM_loop(data, number_of_unrollings, initial_output, initial_state):\n",
    "        outputs = list()\n",
    "        output = initial_output\n",
    "        state = initial_state\n",
    "        for i in range(number_of_unrollings):\n",
    "            output, state = forward_LSTM_cell(data[:,i], output, state)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def backward_LSTM_loop(data, number_of_unrollings, initial_output, initial_state):\n",
    "        outputs = list()\n",
    "        output = initial_output\n",
    "        state = initial_state\n",
    "        for i in range(number_of_unrollings):\n",
    "            output, state = backward_LSTM_cell(data[:,-i-1], output, state)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def back_and_forth_concat(forward_outputs, backward_outputs, number_of_unrollings):\n",
    "        outputs = list()\n",
    "        for i in range(number_of_unrollings):\n",
    "            output = tf.concat([forward_outputs[i], backward_outputs[-i-1]], axis = 1)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    # Attention model:\n",
    "    def attention_layer(outputs, W_attn, b_attn, C_attn):\n",
    "        U = list()\n",
    "        for i in range(len(outputs)):\n",
    "            u = tf.tanh(tf.nn.xw_plus_b(outputs[i], W_attn, b_attn))\n",
    "            U.append(u)\n",
    "    \n",
    "        C = list()\n",
    "        for i in range(len(U)):\n",
    "            c = tf.matmul(U[i], C_attn)\n",
    "            C.append(c)\n",
    "    \n",
    "        C_concat = tf.concat(C, 1)\n",
    "        S = tf.nn.softmax(C_concat, 1)\n",
    "        \n",
    "        attn_outputs = list()\n",
    "        for i in range(number_of_unrollings):\n",
    "            attn_output = tf.multiply(outputs[i], S[:,i][:, tf.newaxis])\n",
    "            attn_outputs.append(attn_output)\n",
    "        return attn_outputs, S\n",
    "    \n",
    "    # Model.\n",
    "    forward_outputs = forward_LSTM_loop(T, number_of_unrollings, initial_output, initial_state)\n",
    "    backward_outputs = backward_LSTM_loop(T, number_of_unrollings, initial_output, initial_state)\n",
    "    outputs = back_and_forth_concat(forward_outputs, backward_outputs, number_of_unrollings)\n",
    "    attn_outputs, S = attention_layer(outputs, W_attn, b_attn, C_attn)\n",
    "    \n",
    "    # Compute logits and loss for attention-weighted sum of outputs.\n",
    "    L = tf.nn.xw_plus_b(tf.math.add_n(attn_outputs), W, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = Tlabel, logits=L))\n",
    "    \n",
    "    # Decaying learning rate for optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)    \n",
    "    \n",
    "    # Apply gradient clipping. \n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Training predictions. \n",
    "    train_predict = tf.nn.softmax(L)\n",
    "    \n",
    "    # Test predictions. \n",
    "    \n",
    "    # Initialize output, state at origin. \n",
    "    test_initial_output = tf.Variable(tf.zeros([len(test_total), number_of_nodes]), trainable = False)\n",
    "    test_initial_state = tf.Variable(tf.zeros([len(test_total), number_of_nodes]), trainable = False)\n",
    "    \n",
    "    test_forward_outputs = forward_LSTM_loop(E, number_of_unrollings, test_initial_output, test_initial_state)\n",
    "    test_backward_outputs = backward_LSTM_loop(E, number_of_unrollings, test_initial_output, test_initial_state)\n",
    "    test_outputs = back_and_forth_concat(test_forward_outputs, test_backward_outputs, number_of_unrollings)\n",
    "    test_attn_outputs, _ = attention_layer(test_outputs, W_attn, b_attn, C_attn)\n",
    "    test_predict = tf.nn.softmax(tf.nn.xw_plus_b(tf.math.add_n(test_attn_outputs), W, b))\n",
    "    \n",
    "    # Example generator.\n",
    "    evaluation_example = tf.constant(example_data)\n",
    "    \n",
    "    example_initial_output = tf.Variable(tf.zeros([1, number_of_nodes]), trainable = False)\n",
    "    example_initial_state = tf.Variable(tf.zeros([1, number_of_nodes]), trainable = False)\n",
    "    \n",
    "    example_forward_outputs = forward_LSTM_loop(evaluation_example, number_of_unrollings, example_initial_output, example_initial_state)\n",
    "    example_backward_outputs = backward_LSTM_loop(evaluation_example, number_of_unrollings, example_initial_output, example_initial_state)\n",
    "    example_outputs = back_and_forth_concat(example_forward_outputs, example_backward_outputs, number_of_unrollings)\n",
    "    example_attn_outputs, S_example = attention_layer(example_outputs, W_attn, b_attn, C_attn)\n",
    "    example_predict = tf.nn.softmax(tf.nn.xw_plus_b(tf.math.add_n(example_attn_outputs), W, b))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1001/1001 [00:17<00:00, 56.09it/s]\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations = 1001\n",
    "data_index = 0\n",
    "\n",
    "with tf.Session(graph=BiLSTM_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for iteration in tqdm(range(number_of_iterations)):\n",
    "        batch_data, batch_labels = generate_LSTM_batch(train_data, \n",
    "                                                       train_labels, \n",
    "                                                       batch_size, \n",
    "                                                       number_of_unrollings, \n",
    "                                                       embedding_dimension)\n",
    "        feed_dict = {T : batch_data, Tlabel : batch_labels}\n",
    "        _, l, train_predictions, lr = session.run([optimizer, loss, train_predict, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if iteration == 10:\n",
    "            ten_attention_coeff = S.eval(feed_dict = feed_dict)\n",
    "        \n",
    "    last_attention_coeff = S.eval(feed_dict = feed_dict)\n",
    "    \n",
    "    test = False\n",
    "    if test:\n",
    "        test_predictions = test_predict.eval()\n",
    "        test_acc1 = first_order_accuracy(test_predictions, test_total_labels)\n",
    "        test_acc2 = second_order_accuracy(test_predictions, test_total_labels)\n",
    "        print('====== Test Report ======')\n",
    "        print('First-order accuracy: %.1f%%' % test_acc1)\n",
    "        print('Second-order accuracy: %.1f%%' % test_acc2)\n",
    "        print('Accuracy when first and second choice are counted: %.1f%%' % (test_acc1+test_acc2))\n",
    "    \n",
    "    # Evaluate the example. \n",
    "    example_predict = example_predict.eval()\n",
    "    example_attn = S_example.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== EXAMPLE ====\n",
      "\n",
      "Text: naratriptan rx - pls sign on 3 / 3 request <end>\n",
      "True Label: [0. 0. 0. 0. 1.] = PRESCRIPTION\n",
      "Prediction Score: [0.006, 0.022, 0.001, 0.006, 0.965]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.003, 0.004, 0.011, 0.018, 0.851, 0.02, 0.055, 0.001, 0.025, 0.003, 0.0, 0.001, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"==== EXAMPLE ====\", end = \"\\n\\n\")\n",
    "print(\"Text:\", end = \" \")\n",
    "for i in range(len(example_sentence)):\n",
    "        if reverse_dictionary[example_sentence[i]] != \"\":\n",
    "            print(reverse_dictionary[example_sentence[i]], end = \" \")\n",
    "print(\"<end>\")\n",
    "print(\"True Label:\", end = \" \")\n",
    "print(example_label.reshape(5), end = \" = \")\n",
    "print(vec2label(example_label.reshape(5)))\n",
    "print(\"Prediction Score:\", end = \" \")\n",
    "print([round(entry, 3) for entry in example_predict.reshape(5)])\n",
    "print([round(entry, 3) for entry in example_attn.reshape(28)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
